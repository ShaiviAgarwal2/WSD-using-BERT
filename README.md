# WSD-using-BERT
Word sense Disambiguation (WSD) is a Natural language processing (NLP) tool that involves determining the correct meaning of the word based on its context. Many words in languages, especially in English are polysemous meaning they have multiple meanings based on how they are used. The goal of WSD is to accurately predict the intended sense of a word in a particular sentence or text, which is crucial for task like machine translation, information retrieval and chatbots, etc.
Example: “I went to the bank to deposit money.” (Here “bank” means a financial institution) and “The children played by the bank of the river.” (Here “bank” means the land beside a river)

<h3>Key Responsibilities:</h3> 
<ul>
<li>Designed and implemented a context-aware disambiguation system using BERT and WordNet-based datasets.</li>
<li>Preprocessed and annotated datasets with correct word senses to train and evaluate disambiguation models.</li>
<li>Integrated transformer-based models (like BERT) with classical NLP techniques to improve accuracy in predicting word senses.</li>
<li>Developed a user interface that allowed users to input sentences and target words, displaying the predicted sense with explanation.</li>
</ul>

<h3>Achievements:</h3>
Achieved 73.8% accuracy on the SemCor validation set using a fine-tuned BERT model, outperforming the Most Frequent Sense (MFS) baseline of 67.5% by 6.3 percentage points.
